{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word2id(save_to_path=None):\n",
    "    \"\"\"\n",
    "    :param save_to_path: path to save word2id\n",
    "    :return: word2id dictionary {word: id}\n",
    "    \"\"\"\n",
    "    word2id = {'_PAD_': 0}\n",
    "    path = ['./Dataset/train.txt', './Dataset/validation.txt']\n",
    "    \n",
    "    # write the index to word2id[word]\n",
    "    for _path in path:\n",
    "        with open(_path, encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                sp = line.strip().split()\n",
    "                for word in sp[1:]:\n",
    "                    if word not in word2id.keys():\n",
    "                        word2id[word] = len(word2id)\n",
    "    if save_to_path:                    \n",
    "        with open(save_to_path, 'w', encoding='utf-8') as f:\n",
    "            for w in word2id:\n",
    "                f.write(w+'\\t')\n",
    "                f.write(str(word2id[w]))\n",
    "                f.write('\\n')\n",
    "    \n",
    "    return word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 58954\n"
     ]
    }
   ],
   "source": [
    "word2id = build_word2id('./Dataset/word2id.txt')\n",
    "print(type(word2id), len(word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word2vec(fname, word2id, save_to_path=None):\n",
    "    \"\"\"\n",
    "    :param fname: pre-trained word2vec by others\n",
    "    :param word2id: built word2id by us\n",
    "    :param save_to_path: path to save word2vec\n",
    "    :return: wordid_vecs means wordid to wordvector dictionary {id: word2vec}\n",
    "    \"\"\"\n",
    "    n_words = max(word2id.values()) + 1\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True)\n",
    "    wordid_vecs = np.array(np.random.uniform(-1., 1., [n_words, model.vector_size]))\n",
    "    for word in word2id.keys():\n",
    "        try:\n",
    "            wordid_vecs[word2id[word]] = model[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if save_to_path:\n",
    "        with open(save_to_path, 'w', encoding='utf-8') as f:\n",
    "            for vec in wordid_vecs:\n",
    "                vec = [str(w) for w in vec]\n",
    "                f.write(' '.join(vec))\n",
    "                f.write('\\n')\n",
    "    return wordid_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.78658611  0.02278571  0.26487929 ... -0.3351795  -0.91272504\n",
      "   0.70551549]\n",
      " [ 0.33509246 -0.59427804  0.17512511 ... -0.03102478  0.0637431\n",
      "   0.07115829]\n",
      " [ 0.59741241  0.75101197  0.18067721 ... -0.18746521  0.6136632\n",
      "  -0.12780823]\n",
      " ...\n",
      " [ 0.18475929  0.96427193 -0.14187175 ...  0.12656132 -0.64530415\n",
      "   0.38892012]\n",
      " [ 0.2875104   0.22257346  0.6065948  ... -0.10173644  0.16712433\n",
      "   0.191283  ]\n",
      " [ 0.50211847 -0.45644817  0.21553677 ... -0.34367573  0.1960599\n",
      "  -0.03544554]]\n"
     ]
    }
   ],
   "source": [
    "word2vec = build_word2vec('./Dataset/wiki_word2vec_50.bin', word2id)\n",
    "assert word2vec.shape == (58954, 50)\n",
    "print(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_to_id(classes=None):\n",
    "    \"\"\"\n",
    "    :param classes: label to classify, default is 0:pos, 1:neg\n",
    "    :return: classes = ['0', '1'], {classes：id} = {'0': 0, '1': 1}\n",
    "    \"\"\"\n",
    "    if not classes:\n",
    "        classes = ['0', '1']\n",
    "    clas2id = {clas: idx for (idx, clas) in enumerate(classes)}\n",
    "    return classes, clas2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(path, word2id, max_sen_len=50):\n",
    "    \"\"\"\n",
    "    :param \n",
    "        path: sample corpus file\n",
    "        word2id: built word2id by us\n",
    "    :return\n",
    "        contents: array, text to id;\n",
    "        labels_arr: array, (len,)\n",
    "        labels_onehot: array, onehot format, (len, 2)\n",
    "    \"\"\"\n",
    "    _, clas2id = class_to_id()\n",
    "    contents, labels = [], []\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            sp = line.strip().split()\n",
    "            # print(sp)\n",
    "            label = sp[0]\n",
    "            content = [word2id.get(w, 0) for w in sp[1:]]\n",
    "            content = content[:max_sen_len]\n",
    "            if len(content) < max_sen_len:\n",
    "                content += [word2id['_PAD_']] * (max_sen_len - len(content))\n",
    "            labels.append(label)\n",
    "            contents.append(content)\n",
    "    counter = Counter(labels)\n",
    "    print('总样本数为：%d' % (len(labels)))\n",
    "    print('各个类别样本数如下：')\n",
    "    for w in counter:\n",
    "        print(w, counter[w])\n",
    "\n",
    "    contents = np.asarray(contents)\n",
    "    \n",
    "    labels_arr = np.array([clas2id[l] for l in labels])\n",
    "    \n",
    "    labels_onehot = np.array([[0,0]] * len(labels))\n",
    "    for idx, val in enumerate(labels):\n",
    "        if val == '0':\n",
    "            labels_onehot[idx][0]=1\n",
    "        else:\n",
    "            labels_onehot[idx][1]=1\n",
    "\n",
    "    return contents, labels_arr, labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train corpus load: \n",
      "总样本数为：19998\n",
      "各个类别样本数如下：\n",
      "1 9999\n",
      "0 9999\n",
      "\n",
      "validation corpus load: \n",
      "总样本数为：5629\n",
      "各个类别样本数如下：\n",
      "1 2812\n",
      "0 2817\n",
      "\n",
      "test corpus load: \n",
      "总样本数为：369\n",
      "各个类别样本数如下：\n",
      "1 187\n",
      "0 182\n"
     ]
    }
   ],
   "source": [
    "print('train corpus load: ')\n",
    "train_contents, train_labels, _ = load_corpus('./Dataset/train.txt', word2id, max_sen_len=50)\n",
    "print('\\nvalidation corpus load: ')\n",
    "val_contents, val_labels, _ = load_corpus('./Dataset/validation.txt', word2id, max_sen_len=50)\n",
    "print('\\ntest corpus load: ')\n",
    "test_contents, test_labels, _ = load_corpus('./Dataset/test.txt', word2id, max_sen_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG():\n",
    "    update_w2v = True           # update w2v during training or not\n",
    "    vocab_size = 58954          # vocabulary size same as the # of word2id\n",
    "    n_class = 2                 # the # of classes\n",
    "    embedding_dim = 50          # dimension of wordvector\n",
    "    drop_keep_prob = 0.5        # dropout layer, the rate of parameter 'keep'\n",
    "    num_filters = 256           # the # of filter in convolution layer\n",
    "    kernel_size = 3             # the size of kernel in convolution layer\n",
    "    pretrained_embed = word2vec # pretrained word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TextCNN, self).__init__()\n",
    "        update_w2v = config.update_w2v\n",
    "        vocab_size = config.vocab_size\n",
    "        n_class = config.n_class\n",
    "        embedding_dim = config.embedding_dim\n",
    "        num_filters = config.num_filters\n",
    "        kernel_size = config.kernel_size\n",
    "        drop_keep_prob = config.drop_keep_prob\n",
    "        pretrained_embed = config.pretrained_embed\n",
    "        \n",
    "        # Use the pre-trained wordvector\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embed))\n",
    "        self.embedding.weight.requires_grad = update_w2v\n",
    "        # Convolution layer\n",
    "        self.conv = nn.Conv2d(1,num_filters,(kernel_size,embedding_dim))\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(drop_keep_prob)\n",
    "        # Full connection layer\n",
    "        self.fc = nn.Linear(num_filters, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.int64)\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.relu(self.conv(x)).squeeze(3)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "config = CONFIG()          # config the parameters of model\n",
    "learning_rate = 0.001      # learn rate     \n",
    "batch_size = 32            # batch size\n",
    "epochs = 4                 # epoches\n",
    "model_path = None          # path of pre-trained model\n",
    "verbose = True             # print the training process\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix the contents & labels of train & validation dataset as train_dataloader\n",
    "contents = np.vstack([train_contents, val_contents])\n",
    "labels = np.concatenate([train_labels, val_labels])\n",
    "train_dataset = TensorDataset(torch.from_numpy(contents).type(torch.float), \n",
    "                              torch.from_numpy(labels).type(torch.long))\n",
    "train_dataloader = DataLoader(dataset = train_dataset, batch_size = batch_size, \n",
    "                              shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "\n",
    "    # config the model, load the pretrained model if model_path\n",
    "    model = TextCNN(config)\n",
    "    if model_path:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    \n",
    "    # set optimizer & loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # circuit train\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(dataloader):\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "            \n",
    "            if batch_idx % 200 == 0 & verbose:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch+1, batch_idx * len(batch_x), len(dataloader.dataset),\n",
    "                    100. * batch_idx / len(dataloader), loss.item()))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # save model\n",
    "    torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/25627 (0%)]\tLoss: 0.629151\n",
      "Train Epoch: 1 [6400/25627 (25%)]\tLoss: 0.551297\n",
      "Train Epoch: 1 [12800/25627 (50%)]\tLoss: 0.490176\n",
      "Train Epoch: 1 [19200/25627 (75%)]\tLoss: 0.401468\n",
      "Train Epoch: 1 [21600/25627 (100%)]\tLoss: 0.305098\n",
      "Train Epoch: 2 [0/25627 (0%)]\tLoss: 0.258761\n",
      "Train Epoch: 2 [6400/25627 (25%)]\tLoss: 0.478694\n",
      "Train Epoch: 2 [12800/25627 (50%)]\tLoss: 0.271887\n",
      "Train Epoch: 2 [19200/25627 (75%)]\tLoss: 0.200100\n",
      "Train Epoch: 2 [21600/25627 (100%)]\tLoss: 0.232280\n",
      "Train Epoch: 3 [0/25627 (0%)]\tLoss: 0.235382\n",
      "Train Epoch: 3 [6400/25627 (25%)]\tLoss: 0.191643\n",
      "Train Epoch: 3 [12800/25627 (50%)]\tLoss: 0.159345\n",
      "Train Epoch: 3 [19200/25627 (75%)]\tLoss: 0.397158\n",
      "Train Epoch: 3 [21600/25627 (100%)]\tLoss: 0.303639\n",
      "Train Epoch: 4 [0/25627 (0%)]\tLoss: 0.126981\n",
      "Train Epoch: 4 [6400/25627 (25%)]\tLoss: 0.074152\n",
      "Train Epoch: 4 [12800/25627 (50%)]\tLoss: 0.162100\n",
      "Train Epoch: 4 [19200/25627 (75%)]\tLoss: 0.063554\n",
      "Train Epoch: 4 [21600/25627 (100%)]\tLoss: 0.171760\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test parameters\n",
    "model_path = 'model.pth'\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset as test_dataloader\n",
    "test_dataset = TensorDataset(torch.from_numpy(test_contents).type(torch.float), \n",
    "                            torch.from_numpy(test_labels).type(torch.long))\n",
    "test_dataloader = DataLoader(dataset = test_dataset, batch_size = batch_size, \n",
    "                            shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataloader):\n",
    "\n",
    "    # load trained model\n",
    "    model = TextCNN(config)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # circuit test\n",
    "    count, correct, real_predict_00, real_predict_01, real_predict_10, real_predict_11 = 0, 0, 0, 0, 0, 0\n",
    "    for _, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        output = model(batch_x)\n",
    "        count += len(batch_x)\n",
    "        correct += (output.argmax(1) == batch_y).float().sum().item()\n",
    "#         print(np.array([(output.argmax(1)[idx] == 0 and batch_y[idx] == 0).float().cpu().numpy() for (idx, _) in enumerate(batch_y)]))\n",
    "#         print([(output.argmax(1)[idx] == 0 and batch_y[idx] == 0).float().cpu().numpy() for (idx, _) in enumerate(batch_y)])\n",
    "        real_predict_00 += np.array([(output.argmax(1)[idx] == 0 and batch_y[idx] == 0).float().cpu().numpy() for (idx, _) in enumerate(batch_y)]).sum().item()\n",
    "        real_predict_01 += np.array([(output.argmax(1)[idx] == 0 and batch_y[idx] == 1).float().cpu().numpy() for (idx, _) in enumerate(batch_y)]).sum().item()\n",
    "        real_predict_10 += np.array([(output.argmax(1)[idx] == 1 and batch_y[idx] == 0).float().cpu().numpy() for (idx, _) in enumerate(batch_y)]).sum().item()\n",
    "        real_predict_11 += np.array([(output.argmax(1)[idx] == 1 and batch_y[idx] == 1).float().cpu().numpy() for (idx, _) in enumerate(batch_y)]).sum().item()\n",
    "    \n",
    "    # calculate accuracy, precision, recall, F1_score, confusion_matrix\n",
    "    accuracy = correct/count\n",
    "    precision = real_predict_00 / (real_predict_00 + real_predict_10)\n",
    "    recall = real_predict_00 / (real_predict_00 + real_predict_01)\n",
    "    F1_score = 2*precision*recall/(precision+recall)\n",
    "    confusion_matrix = [[real_predict_00, real_predict_01], [real_predict_10, real_predict_11]]\n",
    "    print('The accuracy, precision, recall, F1_score, confusion_matrix of test is\\n{:.2f}% \\n{} \\n{} \\n{} \\n{}.'.format(100*accuracy, precision, recall, F1_score, confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy, precision, recall, F1_score, confusion_matrix of test is\n",
      "86.18% \n",
      "0.8406593406593407 \n",
      "0.8742857142857143 \n",
      "0.8571428571428572 \n",
      "[[153.0, 22.0], [29.0, 165.0]].\n"
     ]
    }
   ],
   "source": [
    "predict(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
