{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.Set some hyperparameters\n",
    "Batch_size = 16                                                          # batch size\n",
    "learning_rate = 5e-3                                                     # learn rate\n",
    "embedding_dim = 128                                                      # embedding layer dimension\n",
    "hidden_dim = 256                                                         # hidden layer dimension\n",
    "epochs = 4                                                               # epochs to train\n",
    "verbose = True                                                           # print training process\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  # use GPU first\n",
    "pre_trained_model_path = None                                            # pre_trained model path\n",
    "trained_model_path = 'model.pth'                                         # trained model path\n",
    "start_words = '湖光秋月两相和'                                           # the first sentence of poetry\n",
    "start_words_acrostic = '轻舟已过万重山'                                  # the 'head' the the genrated acrostic\n",
    "max_gen_len = 128                                                        # the max length of generated poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Load data from tang.npz\n",
    "def prepareData():\n",
    "    \n",
    "    # Load Tang poetry data including 3 parts: data, ix2word, word2ix\n",
    "    datas = np.load(\"tang.npz\", allow_pickle=True)\n",
    "    data = datas['data']\n",
    "    ix2word = datas['ix2word'].item()\n",
    "    word2ix = datas['word2ix'].item()\n",
    "    \n",
    "    # Translate data from np to torch.Tensor & generate dataloader\n",
    "    data = torch.from_numpy(data)\n",
    "    print(data.shape) # [57580, 125]\n",
    "    dataloader = DataLoader(data,\n",
    "                         batch_size = Batch_size,\n",
    "                         shuffle = True,\n",
    "                         num_workers = 2)\n",
    "    print(len(dataloader)) # 3599\n",
    "    \n",
    "    return dataloader, ix2word, word2ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57580, 125])\n",
      "3599\n"
     ]
    }
   ],
   "source": [
    "dataloader, ix2word, word2ix = prepareData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Define PoetryModel class\n",
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_dim):\n",
    "        super(PoetryModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=2)\n",
    "        self.linear = nn.Linear(self.hidden_dim, num_embeddings)\n",
    "\n",
    "    def forward(self, input, hidden = None):\n",
    "        seq_len, batch_size = input.size()\n",
    "        \n",
    "        if hidden is None:\n",
    "            h_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n",
    "            c_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n",
    "        else:\n",
    "            h_0, c_0 = hidden\n",
    "\n",
    "        embeds = self.embedding(input)\n",
    "        output, hidden = self.lstm(embeds, (h_0, c_0))\n",
    "        output = self.linear(output.view(seq_len * batch_size, -1))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Define train function\n",
    "def train(dataloader, ix2word, word2ix):\n",
    "    # print(len(dataloader)) # 3599, so all len=16*3598+12*3599=57580, shape=57580*125\n",
    "\n",
    "    # config model & load pre-trained model or not\n",
    "    model = PoetryModel(len(word2ix), embedding_dim, hidden_dim)\n",
    "    if pre_trained_model_path:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    \n",
    "    # set optimizer & loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # circuit train\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            data = data.long().transpose(1, 0).contiguous()\n",
    "            data = data.to(device)\n",
    "            input, target = data[:-1, :], data[1:, :]\n",
    "            output, _ = model(input)\n",
    "            loss = criterion(output, target.view(-1))\n",
    "            \n",
    "            if (batch_idx+1) % 899 == 0 & verbose:\n",
    "                # print(data.shape)  # [125,16]\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch+1, (batch_idx+1) * Batch_size, len(dataloader.dataset),\n",
    "                    100. * (batch_idx+1) / len(dataloader), loss.item()))\n",
    "            # if batch_idx==3598:\n",
    "            #     print(data.shape)  # [125,12]\n",
    "                \n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # save model\n",
    "    torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [14384/57580 (25%)]\tLoss: 2.048638\n",
      "Train Epoch: 1 [28768/57580 (50%)]\tLoss: 2.725324\n",
      "Train Epoch: 1 [43152/57580 (75%)]\tLoss: 2.032657\n",
      "Train Epoch: 1 [57536/57580 (100%)]\tLoss: 2.049974\n",
      "Train Epoch: 2 [14384/57580 (25%)]\tLoss: 1.910754\n",
      "Train Epoch: 2 [28768/57580 (50%)]\tLoss: 2.663542\n",
      "Train Epoch: 2 [43152/57580 (75%)]\tLoss: 2.669175\n",
      "Train Epoch: 2 [57536/57580 (100%)]\tLoss: 1.686005\n",
      "Train Epoch: 3 [14384/57580 (25%)]\tLoss: 1.997194\n",
      "Train Epoch: 3 [28768/57580 (50%)]\tLoss: 1.950005\n",
      "Train Epoch: 3 [43152/57580 (75%)]\tLoss: 1.966352\n",
      "Train Epoch: 3 [57536/57580 (100%)]\tLoss: 2.264234\n",
      "Train Epoch: 4 [14384/57580 (25%)]\tLoss: 1.941731\n",
      "Train Epoch: 4 [28768/57580 (50%)]\tLoss: 2.044527\n",
      "Train Epoch: 4 [43152/57580 (75%)]\tLoss: 1.607541\n",
      "Train Epoch: 4 [57536/57580 (100%)]\tLoss: 1.713295\n"
     ]
    }
   ],
   "source": [
    "train(dataloader, ix2word, word2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Define generate poetry function\n",
    "def generate(start_words, ix2word, word2ix):\n",
    "\n",
    "    # load trained_model from trained_model_path\n",
    "    model = PoetryModel(len(word2ix), embedding_dim, hidden_dim)\n",
    "    model.load_state_dict(torch.load(trained_model_path))\n",
    "    model.to(device)\n",
    "    \n",
    "    # list the start sentence\n",
    "    results = list(start_words)\n",
    "    start_word_len = len(start_words)\n",
    "    \n",
    "    # set the first word as <START>\n",
    "    input = torch.Tensor([word2ix['<START>']]).view(1, 1).long()\n",
    "    input = input.to(device)\n",
    "    hidden = None\n",
    "\n",
    "    # generate poetry in the range of max_gen_len\n",
    "    for i in range(max_gen_len):\n",
    "        output, hidden = model(input, hidden)\n",
    "        # print(len(output[0]),len(hidden[0]),len(hidden[1])) # 1*8293, 2*2\n",
    "        # load start_words as the first sentence\n",
    "        if i < start_word_len:\n",
    "            w = results[i]\n",
    "            input = input.data.new([word2ix[w]]).view(1, 1)\n",
    "        # generate other sentences\n",
    "        else:\n",
    "            top_index = output.data[0].topk(1)[1][0].item()\n",
    "            w = ix2word[top_index]\n",
    "            results.append(w)\n",
    "            input = input.data.new([top_index]).view(1, 1)\n",
    "        # end label '<EOP>'\n",
    "        if w == '<EOP>':\n",
    "            del results[-1]\n",
    "            break\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['湖', '光', '秋', '月', '两', '相', '和', '，', '一', '片', '云', '山', '无', '一', '声', '。', '一', '朝', '不', '见', '青', '山', '曲', '，', '不', '见', '人', '间', '无', '一', '人', '。', '一', '朝', '不', '见', '青', '山', '曲', '，', '不', '见', '东', '风', '吹', '白', '云', '。', '一', '朝', '不', '见', '青', '山', '曲', '，', '不', '见', '东', '风', '吹', '白', '云', '。', '一', '朝', '不', '见', '青', '山', '曲', '，', '不', '见', '东', '风', '吹', '白', '云', '。', '一', '朝', '不', '见', '青', '山', '曲', '，', '不', '见', '青', '山', '不', '可', '见', '。', '一', '朝', '不', '见', '青', '山', '人', '，', '不', '见', '春', '风', '吹', '白', '雪', '。', '一', '朝', '不', '见', '春', '风', '起', '，', '一', '曲', '花', '前', '花', '下', '来', '。']\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "results = generate(start_words, ix2word, word2ix)\n",
    "print(results)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Define generate acrostic function\n",
    "def gen_acrostic(start_words, ix2word, word2ix):\n",
    "\n",
    "    # load trained_model from trained_model_path\n",
    "    model = PoetryModel(len(word2ix), embedding_dim, hidden_dim)\n",
    "    model.load_state_dict(torch.load(trained_model_path))\n",
    "    model.to(device)\n",
    "    \n",
    "    # load the 'head' of the acrostic\n",
    "    results = []\n",
    "    start_word_len = len(start_words)\n",
    "    \n",
    "    # set the first word as <START>\n",
    "    input = (torch.Tensor([word2ix['<START>']]).view(1, 1).long())\n",
    "    input = input.to(device)\n",
    "    hidden = None\n",
    "\n",
    "    index = 0            # index of the character in start_words\n",
    "    pre_word = '<START>' # pre_word\n",
    "\n",
    "    # generate acrostic in the range of max_gen_len\n",
    "    for i in range(max_gen_len):\n",
    "        output, hidden = model(input, hidden)\n",
    "        top_index = output.data[0].topk(1)[1][0].item()\n",
    "        w = ix2word[top_index]\n",
    "\n",
    "        # if the pre_word is end or start label, set the next character in start_words as the next word \n",
    "        if (pre_word in {u'。', u'！', '<START>'}):\n",
    "            # condition of end\n",
    "            if index == start_word_len:\n",
    "                break\n",
    "            # feed the next character as head\n",
    "            else:\n",
    "                w = start_words[index]\n",
    "                index += 1\n",
    "                input = (input.data.new([word2ix[w]])).view(1, 1)\n",
    "        # otherwise, set the next prediction as the next word \n",
    "        else:\n",
    "            input = (input.data.new([word2ix[w]])).view(1, 1)\n",
    "            \n",
    "        results.append(w)\n",
    "        pre_word = w\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['轻', '生', '不', '得', '意', '，', '不', '得', '不', '得', '知', '。', '舟', '中', '有', '奇', '气', '，', '不', '得', '不', '得', '持', '。', '已', '闻', '天', '上', '来', '，', '不', '得', '不', '得', '宁', '。', '过', '此', '不', '可', '见', '，', '不', '知', '何', '处', '期', '。', '万', '里', '不', '可', '见', '，', '一', '朝', '无', '人', '知', '。', '重', '阳', '不', '可', '见', '，', '一', '日', '不', '可', '攀', '。', '山', '川', '有', '高', '树', '，', '山', '水', '无', '人', '舟', '。']\n"
     ]
    }
   ],
   "source": [
    "results_acrostic = gen_acrostic(start_words_acrostic, ix2word, word2ix)\n",
    "print(results_acrostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
